"""
ğŸš€ CHAPTER 2: THE PERCEPTRON (1957) - The First Learning Machine!
================================================================

Historical Context:
- Frank Rosenblatt at Cornell Aeronautical Laboratory
- 1957: First machine that could LEARN from experience!
- New York Times headline: "New Navy Device Learns By Doing"
- The birth of machine learning!

The Revolutionary Breakthrough:
âŒ McCulloch-Pitts: Weights are FIXED forever
âœ… Perceptron: Weights can CHANGE and IMPROVE!

The Magic Question Rosenblatt Asked:
"What if the neuron could adjust its 'importance levels' (weights)
based on whether it makes correct or wrong decisions?"

This is like learning from your mistakes! ğŸ¯
"""

import numpy as np
import matplotlib.pyplot as plt


class LearningPerceptron:
    """
    The revolutionary Perceptron that can LEARN!

    The breakthrough: Instead of fixed weights, it can:
    1. Make a prediction
    2. See if it was right or wrong
    3. Adjust its weights to do better next time!
    """

    def __init__(self, learning_rate=0.1):
        """
        Initialize a learning perceptron

        Args:
            learning_rate: How fast it learns (0.1 = learns slowly but safely)
        """
        self.weights = None
        self.bias = None
        self.learning_rate = learning_rate
        self.errors_history = []

    def predict(self, inputs):
        """
        Make a prediction (same as McCulloch-Pitts)
        """
        # Calculate weighted sum + bias
        weighted_sum = np.dot(inputs, self.weights) + self.bias

        # Apply step function (threshold at 0)
        return 1 if weighted_sum >= 0 else 0

    def learn_from_mistake(self, inputs, expected_output, actual_output):
        """
        ğŸ¯ THE MAGIC LEARNING RULE!

        This is Rosenblatt's breakthrough algorithm:
        - If prediction is CORRECT â†’ don't change anything
        - If prediction is WRONG â†’ adjust weights in right direction
        """
        error = expected_output - actual_output

        if error != 0:  # Only learn if we made a mistake!
            print(f"   âŒ MISTAKE! Expected {expected_output}, got {actual_output}")
            print(f"   ğŸ”§ Adjusting weights...")

            # Rosenblatt's learning rule:
            # If we should have said YES but said NO â†’ increase weights
            # If we should have said NO but said YES â†’ decrease weights
            for i in range(len(self.weights)):
                old_weight = self.weights[i]
                self.weights[i] += self.learning_rate * error * inputs[i]
                print(f"      Weight[{i}]: {old_weight:.2f} â†’ {self.weights[i]:.2f}")

            # Also adjust bias
            old_bias = self.bias
            self.bias += self.learning_rate * error
            print(f"      Bias: {old_bias:.2f} â†’ {self.bias:.2f}")
        else:
            print(f"   âœ… CORRECT! Expected {expected_output}, got {actual_output}")

    def train(self, training_inputs, training_outputs, epochs=10):
        """
        Train the perceptron on examples

        Args:
            training_inputs: Examples to learn from
            training_outputs: Correct answers
            epochs: How many times to go through all examples
        """
        # Initialize random weights (small values)
        num_inputs = len(training_inputs[0])
        self.weights = np.random.normal(0, 0.1, num_inputs)
        self.bias = np.random.normal(0, 0.1)

        print(f"ğŸ¯ Starting training with random weights: {self.weights}")
        print(f"   Initial bias: {self.bias:.2f}")
        print("=" * 60)

        for epoch in range(epochs):
            print(f"\nğŸ“š EPOCH {epoch + 1}:")
            total_errors = 0

            for i, (inputs, expected) in enumerate(
                zip(training_inputs, training_outputs)
            ):
                print(f"\n   Example {i + 1}: inputs={inputs}, expected={expected}")

                # Make prediction
                prediction = self.predict(inputs)

                # Learn from result
                self.learn_from_mistake(inputs, expected, prediction)

                # Track errors
                if prediction != expected:
                    total_errors += 1

            self.errors_history.append(total_errors)
            print(f"\n   ğŸ“Š Epoch {epoch + 1} complete: {total_errors} errors")

            # Stop if perfect!
            if total_errors == 0:
                print(f"   ğŸ‰ PERFECT! Learned everything in {epoch + 1} epochs!")
                break

        print(f"\nğŸ¯ Final weights: {self.weights}")
        print(f"   Final bias: {self.bias:.2f}")


def demonstrate_and_gate_learning():
    """
    Let's watch the perceptron LEARN the AND gate!
    This is the same problem McCulloch-Pitts could solve,
    but now the perceptron figures out the weights BY ITSELF!
    """
    print("ğŸ§  WATCHING THE PERCEPTRON LEARN: AND Gate")
    print("=" * 50)

    print("Teaching the perceptron the AND gate:")
    print("   Input [0,0] should output 0")
    print("   Input [0,1] should output 0")
    print("   Input [1,0] should output 0")
    print("   Input [1,1] should output 1")
    print("\nLet's see if it can figure out the pattern...")

    # Training data for AND gate
    X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y_train = np.array([0, 0, 0, 1])

    # Create and train perceptron
    perceptron = LearningPerceptron(learning_rate=0.5)
    perceptron.train(X_train, y_train, epochs=10)

    # Test the learned perceptron
    print("\nğŸ§ª TESTING THE LEARNED PERCEPTRON:")
    print("=" * 40)
    for inputs, expected in zip(X_train, y_train):
        prediction = perceptron.predict(inputs)
        status = "âœ…" if prediction == expected else "âŒ"
        print(f"   {inputs} â†’ {prediction} (expected {expected}) {status}")

    return perceptron


def demonstrate_or_gate_learning():
    """
    Now let's teach it the OR gate - different pattern!
    """
    print("\n\nğŸ§  TEACHING A NEW PATTERN: OR Gate")
    print("=" * 45)

    print("Teaching the perceptron the OR gate:")
    print("   Input [0,0] should output 0")
    print("   Input [0,1] should output 1")
    print("   Input [1,0] should output 1")
    print("   Input [1,1] should output 1")

    # Training data for OR gate
    X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y_train = np.array([0, 1, 1, 1])

    # Create and train perceptron
    perceptron = LearningPerceptron(learning_rate=0.5)
    perceptron.train(X_train, y_train, epochs=10)

    # Test the learned perceptron
    print("\nğŸ§ª TESTING THE LEARNED OR GATE:")
    print("=" * 35)
    for inputs, expected in zip(X_train, y_train):
        prediction = perceptron.predict(inputs)
        status = "âœ…" if prediction == expected else "âŒ"
        print(f"   {inputs} â†’ {prediction} (expected {expected}) {status}")

    return perceptron


def the_famous_limitation():
    """
    The limitation that stumped researchers for years!
    """
    print("\n\nâš ï¸  THE FAMOUS XOR PROBLEM")
    print("=" * 35)

    print("Let's try to teach the XOR (exclusive OR) pattern:")
    print("   Input [0,0] should output 0")
    print("   Input [0,1] should output 1")
    print("   Input [1,0] should output 1")
    print("   Input [1,1] should output 0  â† This is the tricky part!")
    print("\nThis means: output 1 if inputs are DIFFERENT")

    # Training data for XOR gate
    X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y_train = np.array([0, 1, 1, 0])

    # Try to train perceptron
    perceptron = LearningPerceptron(learning_rate=0.5)
    perceptron.train(X_train, y_train, epochs=20)

    # Test the result
    print("\nğŸ§ª TESTING XOR LEARNING:")
    print("=" * 25)
    correct = 0
    for inputs, expected in zip(X_train, y_train):
        prediction = perceptron.predict(inputs)
        status = "âœ…" if prediction == expected else "âŒ"
        if prediction == expected:
            correct += 1
        print(f"   {inputs} â†’ {prediction} (expected {expected}) {status}")

    print(f"\nğŸ“Š Accuracy: {correct}/4 = {correct/4*100:.0f}%")

    if correct < 4:
        print("\nâŒ THE PERCEPTRON FAILED TO LEARN XOR!")
        print("This limitation led to the 'AI Winter' of the 1970s...")
        print("But it also led to the next breakthrough: MULTI-LAYER NETWORKS!")


def real_world_example():
    """
    A more realistic example - learning to recognize patterns
    """
    print("\n\nğŸŒŸ REAL-WORLD EXAMPLE: Weather Decision")
    print("=" * 45)

    print("Teaching perceptron when to go outside based on weather:")
    print("Features: [sunny, warm, windy]")
    print("Goal: Learn when conditions are good for going outside")

    # More realistic training data
    # [sunny, warm, windy] â†’ go_outside
    weather_data = np.array(
        [
            [1, 1, 0],  # sunny, warm, not windy â†’ YES
            [1, 0, 0],  # sunny, cold, not windy â†’ NO
            [0, 1, 0],  # cloudy, warm, not windy â†’ NO
            [1, 1, 1],  # sunny, warm, windy â†’ NO (too windy!)
            [0, 0, 0],  # cloudy, cold, not windy â†’ NO
            [1, 0, 1],  # sunny, cold, windy â†’ NO
            [0, 1, 1],  # cloudy, warm, windy â†’ NO
            [0, 0, 1],  # cloudy, cold, windy â†’ NO
        ]
    )

    weather_labels = np.array([1, 0, 0, 0, 0, 0, 0, 0])

    # Train the perceptron
    weather_perceptron = LearningPerceptron(learning_rate=0.3)
    weather_perceptron.train(weather_data, weather_labels, epochs=15)

    # Test on new weather conditions
    print("\nğŸ§ª TESTING ON NEW WEATHER CONDITIONS:")
    print("=" * 40)

    test_conditions = [
        ([1, 1, 0], "Perfect day: sunny, warm, no wind"),
        ([0, 0, 1], "Terrible day: cloudy, cold, windy"),
        ([1, 0, 0], "Sunny but cold"),
    ]

    for conditions, description in test_conditions:
        prediction = weather_perceptron.predict(conditions)
        decision = "Go outside! â˜€ï¸" if prediction == 1 else "Stay inside! ğŸ "
        print(f"   {conditions} ({description}) â†’ {decision}")


def plot_learning_progress(perceptron):
    """
    Visualize how the perceptron's errors decrease over time
    """
    if len(perceptron.errors_history) > 1:
        plt.figure(figsize=(10, 6))
        plt.plot(
            range(1, len(perceptron.errors_history) + 1),
            perceptron.errors_history,
            "bo-",
        )
        plt.xlabel("Epoch")
        plt.ylabel("Number of Errors")
        plt.title("Perceptron Learning Progress")
        plt.grid(True, alpha=0.3)
        plt.show()


if __name__ == "__main__":
    print("ğŸš€ THE PERCEPTRON: First Machine That Could LEARN!")
    print("=" * 55)
    print("Watch as the perceptron figures out patterns BY ITSELF!")
    print()

    # Demonstrate learning AND gate
    and_perceptron = demonstrate_and_gate_learning()

    # Demonstrate learning OR gate
    or_perceptron = demonstrate_or_gate_learning()

    # Show the famous limitation
    the_famous_limitation()

    # Real-world example
    real_world_example()

    print("\nğŸ¯ KEY BREAKTHROUGHS:")
    print("1. âœ… First machine that could LEARN from examples!")
    print("2. âœ… Automatically finds the right weights!")
    print("3. âœ… Gets better with practice!")
    print("4. âŒ BUT... cannot learn non-linear patterns (like XOR)")
    print()
    print("ğŸ”œ NEXT: Multi-Layer Perceptrons - Breaking the Linear Barrier!")
    print("(Finally solving the XOR problem that stumped everyone!)")

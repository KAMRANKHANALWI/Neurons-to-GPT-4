"""
ðŸ§  RNNs: HOW AI GOT MEMORY!
===========================

The Big Problem: Regular neural networks are AMNESIACS!
They see each input independently, with NO memory of what came before.

But what if we need to understand SEQUENCES?
- "I love this movie, it was..." â†’ Need to remember "love" to understand "it"
- Stock prices: Today's price depends on yesterday's trend
- Language: "The cat that I saw yesterday was..." â†’ Remember which "cat"!

Enter RNNs: Neural networks with MEMORY! ðŸ§ ðŸ’­
"""

import numpy as np
import matplotlib.pyplot as plt


def the_memory_problem():
    """
    Show why regular neural networks fail at sequences
    """
    print("ðŸ¤” THE MEMORY PROBLEM")
    print("=" * 25)

    print("ðŸ§  REGULAR NEURAL NETWORKS:")
    print("   Each input is processed INDEPENDENTLY")
    print("   No memory of previous inputs")
    print("   Like having amnesia every second! ðŸ¤•")
    print()

    print("ðŸ“ EXAMPLE: Movie Review Analysis")
    print("   Sentence: 'I love this movie, it was amazing!'")
    print()
    print("   Regular Network processes word by word:")
    print("   Word 1: 'I' â†’ Network: 'Neutral word'")
    print("   Word 2: 'love' â†’ Network: 'Positive word'")
    print("   Word 3: 'this' â†’ Network: 'Neutral word'")
    print("   Word 4: 'movie' â†’ Network: 'Neutral word'")
    print("   Word 5: 'it' â†’ Network: 'What does 'it' refer to??' ðŸ˜µ")
    print()
    print("   âŒ PROBLEM: By word 5, network forgot about 'love' and 'movie'!")
    print("   âŒ Can't understand that 'it' refers to the loved movie!")
    print()

    print("ðŸ’¡ WHAT WE NEED:")
    print("   Memory to connect: 'I love this movie' + 'it was amazing'")
    print("   Understanding: 'it' = the movie that was loved")
    print("   Result: POSITIVE review! âœ…")


def the_rnn_breakthrough():
    """
    Introduce the RNN solution
    """
    print("\n\nðŸš€ THE RNN BREAKTHROUGH!")
    print("=" * 30)

    print("ðŸ’¡ THE GENIUS IDEA:")
    print("   'What if the network could remember what it saw before?'")
    print("   'What if it had a MEMORY that updates with each input?'")
    print()

    print("ðŸ§  RNN = Regular Neural Network + MEMORY!")
    print()
    print("   ðŸ“¥ INPUT: Current word")
    print("   ðŸ§  MEMORY: What I've seen so far")
    print("   ðŸ”„ PROCESS: Combine current input + memory")
    print("   ðŸ“¤ OUTPUT: Prediction + Updated memory")
    print()

    print("ðŸ”„ THE RNN LOOP:")
    print("   Step 1: See 'I' â†’ Memory: 'Someone is speaking'")
    print("   Step 2: See 'love' â†’ Memory: 'Someone loves something'")
    print("   Step 3: See 'this' â†’ Memory: 'Someone loves this thing'")
    print("   Step 4: See 'movie' â†’ Memory: 'Someone loves this movie'")
    print("   Step 5: See 'it' â†’ Memory knows 'it' = the loved movie!")
    print("   Step 6: See 'amazing' â†’ CONCLUSION: Very positive review! ðŸŽ‰")


def rnn_architecture_simple():
    """
    Show the basic RNN architecture
    """
    print("\n\nðŸ—ï¸ RNN ARCHITECTURE: The Memory Machine")
    print("=" * 45)

    print("ðŸ“ BASIC RNN STRUCTURE:")
    print()
    print("   Input(t) â”€â”€â”€â”€â”")
    print("               â†“")
    print("   Memory(t-1) â†’ [RNN CELL] â†’ Output(t)")
    print("               â†“")
    print("               Memory(t) â”€â”€â”")
    print("                          â†“")
    print("                     (saves for next step)")
    print()

    print("ðŸ’¡ KEY INSIGHT:")
    print("   The SAME cell processes each time step")
    print("   But it REMEMBERS what it learned from previous steps!")
    print()

    print("ðŸ”„ UNFOLDED THROUGH TIME:")
    print()
    print("   Input(1)    Input(2)    Input(3)    Input(4)")
    print("      â†“           â†“           â†“           â†“")
    print("   [RNN] â”€â”€â”€â”€> [RNN] â”€â”€â”€â”€> [RNN] â”€â”€â”€â”€> [RNN]")
    print("      â†“           â†“           â†“           â†“")
    print("  Output(1)   Output(2)   Output(3)   Output(4)")
    print()
    print("   Memory flows left to right â†’â†’â†’â†’")
    print("   Same weights used at each step!")


def rnn_math_simple():
    """
    Show the simple math behind RNNs
    """
    print("\n\nðŸ§® RNN MATH: Surprisingly Simple!")
    print("=" * 40)

    print("ðŸŽ¯ THE CORE FORMULA:")
    print()
    print("   Memory(t) = tanh(Input(t) Ã— W_input + Memory(t-1) Ã— W_memory + bias)")
    print("   Output(t) = Memory(t) Ã— W_output")
    print()

    print("ðŸ” BREAKING IT DOWN:")
    print()
    print("   ðŸ“¥ Input(t): Current word (as numbers)")
    print("   ðŸ§  Memory(t-1): What we remembered from before")
    print("   âš–ï¸ W_input: How much to trust current input")
    print("   âš–ï¸ W_memory: How much to trust old memory")
    print("   âž• bias: Adjustment factor")
    print("   ðŸ“ˆ tanh: Keeps values between -1 and +1")
    print()

    print("ðŸ’¡ INTUITION:")
    print("   'Combine what I see NOW with what I REMEMBER'")
    print("   'Create new memory that includes both!'")
    print("   'Use this new memory to make prediction!'")


def step_by_step_example():
    """
    Walk through a concrete example
    """
    print("\n\nðŸ‘£ STEP-BY-STEP EXAMPLE")
    print("=" * 30)

    print("ðŸ“ SENTENCE: 'Cat is happy'")
    print("ðŸŽ¯ TASK: Predict sentiment (positive/negative)")
    print()

    # Simplified example with made-up numbers
    print("ðŸ”¢ SIMPLIFIED NUMBERS:")
    print("   Word embeddings: Cat=0.2, is=0.1, happy=0.8")
    print("   W_input = 0.5, W_memory = 0.3, bias = 0.1")
    print("   Initial memory = 0.0")
    print()

    memory = 0.0
    W_input = 0.5
    W_memory = 0.3
    bias = 0.1

    words = [("Cat", 0.2), ("is", 0.1), ("happy", 0.8)]

    print("ðŸ”„ PROCESSING EACH WORD:")
    print()

    for step, (word, input_val) in enumerate(words, 1):
        print(f"ðŸ“ STEP {step}: '{word}'")
        print(f"   Input: {input_val}")
        print(f"   Previous memory: {memory:.3f}")

        # Calculate new memory (simplified tanh â‰ˆ just the linear part)
        new_memory = input_val * W_input + memory * W_memory + bias
        # Simple activation (instead of tanh)
        new_memory = max(-1, min(1, new_memory))  # Clip between -1 and 1

        print(
            f"   New memory = {input_val} Ã— {W_input} + {memory:.3f} Ã— {W_memory} + {bias}"
        )
        print(f"   New memory = {new_memory:.3f}")

        # Sentiment prediction (positive if > 0.5)
        sentiment = (
            "POSITIVE"
            if new_memory > 0.3
            else "NEGATIVE" if new_memory < -0.3 else "NEUTRAL"
        )
        print(f"   Current sentiment prediction: {sentiment}")
        print()

        memory = new_memory

    print("ðŸŽ‰ FINAL RESULT:")
    print(f"   Final memory: {memory:.3f}")
    print(
        f"   Final prediction: {'POSITIVE! ðŸ˜Š' if memory > 0.3 else 'NEGATIVE ðŸ˜¢' if memory < -0.3 else 'NEUTRAL ðŸ˜'}"
    )
    print()
    print("ðŸ’¡ THE MAGIC:")
    print("   RNN remembered 'Cat' and 'is' while processing 'happy'")
    print("   Built up understanding: 'A cat is in a happy state'")
    print("   Final judgment: POSITIVE sentiment! âœ…")


def rnn_applications():
    """
    Show what RNNs can do
    """
    print("\n\nðŸŒŸ WHAT RNNs REVOLUTIONIZED")
    print("=" * 35)

    applications = [
        (
            "ðŸ“ Language Translation",
            "Input: 'Hello world' â†’ Output: 'Hola mundo'",
            "Remembers context while translating",
        ),
        (
            "ðŸŽµ Music Generation",
            "Input: Previous notes â†’ Output: Next note",
            "Remembers melody and rhythm patterns",
        ),
        (
            "ðŸ“ˆ Stock Prediction",
            "Input: Historical prices â†’ Output: Future price",
            "Remembers price trends and patterns",
        ),
        (
            "ðŸ—£ï¸ Speech Recognition",
            "Input: Audio sequence â†’ Output: Text words",
            "Remembers partial words and context",
        ),
        (
            "ðŸ’¬ Chatbots",
            "Input: User message â†’ Output: Response",
            "Remembers conversation history",
        ),
        (
            "ðŸ“š Text Summarization",
            "Input: Long document â†’ Output: Key points",
            "Remembers important themes throughout",
        ),
    ]

    print("ðŸš€ RNN APPLICATIONS:")
    print()
    for app, example, how in applications:
        print(f"{app}")
        print(f"   Example: {example}")
        print(f"   How: {how}")
        print()

    print("ðŸŽ¯ THE COMMON PATTERN:")
    print("   All involve processing SEQUENCES where ORDER MATTERS!")
    print("   Memory allows understanding of context and dependencies!")


def the_vanishing_gradient_problem():
    """
    Introduce the limitation that led to LSTMs
    """
    print("\n\nâš ï¸ THE VANISHING GRADIENT PROBLEM")
    print("=" * 40)

    print("ðŸ¤” BASIC RNNs HAVE A MEMORY PROBLEM:")
    print("   They can remember recent things well")
    print("   But they FORGET things from long ago!")
    print()

    print("ðŸ“ EXAMPLE PROBLEM:")
    print("   Sentence: 'The cat, which was black and white and very fluffy,")
    print("             and lived in Paris for many years, was sleeping.'")
    print()
    print("   Word 1: 'The cat' â†’ RNN: 'There's a cat' âœ…")
    print("   Word 15: 'was sleeping' â†’ RNN: 'Something was sleeping... but what??' âŒ")
    print()
    print("   ðŸ’” PROBLEM: RNN forgot about the cat!")
    print("   Memory decayed over the long sentence!")
    print()

    print("ðŸ§  WHY THIS HAPPENS:")
    print("   During backpropagation through time:")
    print("   Gradients get smaller and smaller as they flow backward")
    print("   Early words get almost no learning signal!")
    print("   Result: RNN can't learn long-term dependencies ðŸ˜¢")
    print()

    print("ðŸ’¡ THE SOLUTION PREVIEW:")
    print("   This limitation led to LSTM networks!")
    print("   LSTMs have special 'gates' to maintain long-term memory")
    print("   Coming up next in our journey! ðŸš€")


def comparison_with_previous_networks():
    """
    Compare RNNs with what we learned before
    """
    print("\n\nðŸ”— CONNECTION TO PREVIOUS NETWORKS")
    print("=" * 40)

    print("ðŸ“Š NETWORK COMPARISON:")
    print()

    networks = [
        ("Perceptron", "Single decision", "AND/OR gates", "No memory"),
        ("Multi-layer", "Complex decisions", "XOR, classification", "No memory"),
        ("CNN", "Spatial patterns", "Image recognition", "No sequence memory"),
        ("RNN", "Sequential patterns", "Language, time series", "Has memory! ðŸ§ "),
    ]

    print("   Network      | Purpose           | Example Use       | Memory")
    print("   -------------|-------------------|-------------------|----------")
    for name, purpose, example, memory in networks:
        print(f"   {name:<12} | {purpose:<17} | {example:<17} | {memory}")

    print()
    print("ðŸŽ¯ THE PROGRESSION:")
    print("   Perceptron: Learn simple patterns")
    print("   Multi-layer: Learn complex patterns")
    print("   CNN: Learn spatial patterns")
    print("   RNN: Learn temporal/sequential patterns")
    print()
    print("ðŸ’¡ SAME FOUNDATION:")
    print("   All use the same backpropagation learning!")
    print("   All build on McCulloch-Pitts neurons!")
    print("   RNNs just add the MEMORY component! ðŸ”„")


def whats_next():
    """
    Preview what's coming
    """
    print("\n\nðŸ”œ WHAT'S NEXT?")
    print("=" * 15)

    print("âœ… YOU NOW UNDERSTAND:")
    print("   ðŸ§  Regular networks have no memory")
    print("   ðŸ’­ RNNs add memory to process sequences")
    print("   ðŸ”„ Memory updates with each time step")
    print("   ðŸ“ Enables language understanding, prediction")
    print("   âš ï¸ But basic RNNs forget long-term info")
    print()

    print("ðŸš€ COMING UP - LSTMs:")
    print("   ðŸ”’ Gated memory that doesn't vanish")
    print("   ðŸ§  Long-term AND short-term memory")
    print("   ðŸŽ¯ Solve the vanishing gradient problem")
    print("   ðŸ’ª Remember context across long sequences")
    print()

    print("ðŸŽ‰ THEN WE'LL SEE:")
    print("   How LSTMs enabled Google Translate")
    print("   How they powered early chatbots")
    print("   How they led to modern Transformers!")


if __name__ == "__main__":
    print("ðŸ§  RNNs: How AI Got Memory!")
    print("=" * 35)
    print("The breakthrough that made AI understand sequences!")
    print()

    # The memory problem
    the_memory_problem()

    # RNN breakthrough
    the_rnn_breakthrough()

    # Architecture
    rnn_architecture_simple()

    # Simple math
    rnn_math_simple()

    # Step by step example
    step_by_step_example()

    # Applications
    rnn_applications()

    # Vanishing gradient problem
    the_vanishing_gradient_problem()

    # Comparison with previous
    comparison_with_previous_networks()

    # What's next
    whats_next()

    print("\nðŸŒŸ RNN BREAKTHROUGH UNDERSTOOD!")
    print("You now know how AI learned to remember!")
    print("Ready to see how LSTMs solved the long-term memory problem? ðŸ”’âœ¨")

"""
ðŸ§  WHERE DOES ROSENBLATT'S FORMULA COME FROM?
============================================

You asked the PERFECT question! Let's understand exactly WHERE this
magical formula comes from and WHY it's pure genius!

new_weight = old_weight + (learning_rate Ã— error Ã— input)

Let's break it down piece by piece with clear examples!
"""


def the_intuition_behind_formula():
    """
    Start with the basic intuition - why this makes sense
    """
    print("ðŸŽ¯ THE BASIC INTUITION")
    print("=" * 25)

    print("Think of learning to cook pasta:")
    print("ðŸ You try 2 cups water, pasta is too dry")
    print("ðŸ¤” What should you do? ADD MORE WATER!")
    print()
    print("The learning rule is just common sense:")
    print("   If result is TOO LOW â†’ increase the input that caused it")
    print("   If result is TOO HIGH â†’ decrease the input that caused it")
    print()

    print("ðŸ§  For the perceptron:")
    print("   If output should be HIGHER â†’ increase weights")
    print("   If output should be LOWER â†’ decrease weights")


def the_error_part():
    """
    Explain where the error comes from
    """
    print("\n\nðŸŽ¯ PART 1: THE ERROR")
    print("=" * 20)

    print("error = correct_answer - my_prediction")
    print()

    print("ðŸ“Š Let's see what this means:")
    print()

    scenarios = [
        ("I should say YES but said NO", 1, 0, 1),
        ("I should say NO but said YES", 0, 1, -1),
        ("I was correct (YES)", 1, 1, 0),
        ("I was correct (NO)", 0, 0, 0),
    ]

    for description, correct, prediction, error in scenarios:
        print(f"   {description}:")
        print(f"      error = {correct} - {prediction} = {error:+}")
        if error > 0:
            print(f"      â†’ Need to INCREASE output (error is positive)")
        elif error < 0:
            print(f"      â†’ Need to DECREASE output (error is negative)")
        else:
            print(f"      â†’ Perfect! No change needed")
        print()


def the_input_part():
    """
    Explain why we multiply by input
    """
    print("\nðŸŽ¯ PART 2: WHY MULTIPLY BY INPUT?")
    print("=" * 35)

    print("ðŸ¤” Why not just: new_weight = old_weight + error?")
    print()
    print("ðŸ’¡ Because we only want to blame inputs that were ACTIVE!")
    print()

    print("ðŸ• Pizza example:")
    print("   You try a pizza with [good_cheese=1, expensive_price=0]")
    print("   You hate it! (prediction=1, should be 0, error=-1)")
    print()

    print("   Without input multiplication:")
    print("      cheese_weight -= 1  (blame cheese)")
    print("      price_weight -= 1   (blame price too?? But price was 0!)")
    print("   âŒ This is wrong! You can't blame price for something that wasn't there!")
    print()

    print("   With input multiplication:")
    print("      cheese_weight += (-1) Ã— 1 = -1  (blame cheese, it was involved)")
    print(
        "      price_weight += (-1) Ã— 0 = 0    (don't blame price, it wasn't involved)"
    )
    print("   âœ… This makes sense! Only blame what was actually present!")


def the_learning_rate_part():
    """
    Explain the learning rate
    """
    print("\nðŸŽ¯ PART 3: THE LEARNING RATE")
    print("=" * 30)

    print("ðŸ¤” Why not just: new_weight = old_weight + (error Ã— input)?")
    print()
    print("ðŸ’¡ Because we need to control HOW MUCH we change!")
    print()

    print("ðŸš— Think of learning to drive:")
    print("   You're drifting left, need to turn right")
    print()

    print("   Without learning rate:")
    print("      Turn wheel HARD right â†’ overcorrect, now drifting right!")
    print("      Turn wheel HARD left â†’ overcorrect again!")
    print("      â†’ You'll zigzag forever!")
    print()

    print("   With learning rate (0.1 = gentle corrections):")
    print("      Turn wheel slightly right â†’ small correction")
    print("      Still drifting left? Turn slightly more right")
    print("      â†’ Gradually find the perfect steering!")


def the_complete_derivation():
    """
    Show where the formula actually comes from mathematically
    """
    print("\n\nðŸ§® THE MATHEMATICAL DERIVATION")
    print("=" * 35)

    print("Let's say we want to minimize our mistakes.")
    print("Our GOAL: Make the error as small as possible!")
    print()

    print("ðŸŽ¯ Current situation:")
    print("   Weighted sum = inputâ‚Ã—weightâ‚ + inputâ‚‚Ã—weightâ‚‚ + bias")
    print("   Prediction = 1 if weighted_sum â‰¥ 0, else 0")
    print("   Error = correct - prediction")
    print()

    print(
        "ðŸ¤” Question: If error is not zero, which direction should we move the weights?"
    )
    print()

    print("ðŸ’¡ Rosenblatt's insight:")
    print("   If error > 0 (output too low):")
    print("      â†’ We need BIGGER weighted sum")
    print("      â†’ So INCREASE weights for active inputs")
    print()
    print("   If error < 0 (output too high):")
    print("      â†’ We need SMALLER weighted sum")
    print("      â†’ So DECREASE weights for active inputs")
    print()

    print("ðŸ§  The formula emerges naturally:")
    print("   new_weight = old_weight + (small_step Ã— error Ã— input)")
    print("   â†‘                        â†‘           â†‘        â†‘")
    print(
        "   â”‚                        â”‚           â”‚        â””â”€ Only blame active inputs"
    )
    print("   â”‚                        â”‚           â””â”€ Direction to move")
    print("   â”‚                        â””â”€ Don't overshoot")
    print("   â””â”€ Start from current weight")


def step_by_step_example():
    """
    Work through a complete example
    """
    print("\n\nðŸ“ COMPLETE STEP-BY-STEP EXAMPLE")
    print("=" * 40)

    print("ðŸŽ¯ Goal: Learn that [1,1] should output 1 (AND gate)")
    print()
    print("Initial state:")
    weights = [0.2, 0.3]
    bias = 0.1
    learning_rate = 0.5

    print(f"   weights = {weights}")
    print(f"   bias = {bias}")
    print(f"   learning_rate = {learning_rate}")
    print()

    print("ðŸ§® Try input [1,1], should output 1:")

    # Step 1: Calculate prediction
    inputs = [1, 1]
    weighted_sum = inputs[0] * weights[0] + inputs[1] * weights[1] + bias
    prediction = 1 if weighted_sum >= 0 else 0

    print(
        f"   weighted_sum = {inputs[0]}Ã—{weights[0]} + {inputs[1]}Ã—{weights[1]} + {bias}"
    )
    print(f"                = {weighted_sum}")
    print(f"   prediction = {prediction} (since {weighted_sum} >= 0)")
    print()

    # Step 2: Calculate error
    correct = 1
    error = correct - prediction
    print(f"   error = {correct} - {prediction} = {error}")
    print()

    if error != 0:
        print("âŒ Wrong! Let's apply the learning rule:")
        print()

        # Step 3: Apply learning rule
        print("ðŸ“š Applying: new_weight = old_weight + (learning_rate Ã— error Ã— input)")
        print()

        for i in range(len(weights)):
            old_weight = weights[i]
            adjustment = learning_rate * error * inputs[i]
            new_weight = old_weight + adjustment

            print(f"   Weight[{i}]:")
            print(f"      old_weight = {old_weight}")
            print(
                f"      adjustment = {learning_rate} Ã— {error} Ã— {inputs[i]} = {adjustment}"
            )
            print(f"      new_weight = {old_weight} + {adjustment} = {new_weight}")
            print()

            weights[i] = new_weight

        # Bias adjustment
        old_bias = bias
        bias_adjustment = learning_rate * error
        new_bias = old_bias + bias_adjustment

        print(f"   Bias:")
        print(f"      old_bias = {old_bias}")
        print(f"      adjustment = {learning_rate} Ã— {error} = {bias_adjustment}")
        print(f"      new_bias = {old_bias} + {bias_adjustment} = {new_bias}")
        print()

        bias = new_bias

        print(f"ðŸ§  Updated brain state:")
        print(f"   weights = {weights}")
        print(f"   bias = {bias}")
        print()

        # Test again
        print("ðŸ§ª Let's test again with new weights:")
        new_weighted_sum = inputs[0] * weights[0] + inputs[1] * weights[1] + bias
        new_prediction = 1 if new_weighted_sum >= 0 else 0

        print(
            f"   weighted_sum = {inputs[0]}Ã—{weights[0]:.1f} + {inputs[1]}Ã—{weights[1]:.1f} + {bias:.1f}"
        )
        print(f"                = {new_weighted_sum:.1f}")
        print(f"   prediction = {new_prediction}")
        print()

        if new_prediction == correct:
            print("âœ… SUCCESS! Now it's correct!")
        else:
            print("ðŸ”„ Still learning... need more adjustments")


def why_its_genius():
    """
    Explain why this formula is so brilliant
    """
    print("\n\nðŸŽ­ WHY THIS FORMULA IS PURE GENIUS")
    print("=" * 40)

    print("ðŸ† Rosenblatt's formula is brilliant because:")
    print()

    print("1ï¸âƒ£ AUTOMATIC DIRECTION:")
    print("   - Error > 0 â†’ weights go UP (need more activation)")
    print("   - Error < 0 â†’ weights go DOWN (need less activation)")
    print("   - Error = 0 â†’ no change (already perfect!)")
    print()

    print("2ï¸âƒ£ PROPORTIONAL LEARNING:")
    print("   - Big mistake â†’ big adjustment")
    print("   - Small mistake â†’ small adjustment")
    print("   - No mistake â†’ no adjustment")
    print()

    print("3ï¸âƒ£ SELECTIVE BLAME:")
    print("   - Only adjusts weights for inputs that were active")
    print("   - Inactive inputs (0) don't get blamed for mistakes")
    print()

    print("4ï¸âƒ£ CONTROLLABLE SPEED:")
    print("   - Learning rate lets you control how fast to learn")
    print("   - Prevents wild oscillations")
    print()

    print("5ï¸âƒ£ GUARANTEED CONVERGENCE:")
    print("   - For linearly separable problems, this WILL find the solution!")
    print("   - Mathematical proof exists!")


if __name__ == "__main__":
    print("ðŸ§  THE COMPLETE STORY OF ROSENBLATT'S FORMULA")
    print("=" * 55)
    print("new_weight = old_weight + (learning_rate Ã— error Ã— input)")
    print("=" * 55)

    # Build up the intuition
    the_intuition_behind_formula()

    # Explain each part
    the_error_part()
    the_input_part()
    the_learning_rate_part()

    # Mathematical derivation
    the_complete_derivation()

    # Complete example
    step_by_step_example()

    # Why it's genius
    why_its_genius()

    print("\nðŸŽ¯ KEY INSIGHTS:")
    print("âœ… The formula comes from COMMON SENSE about learning!")
    print("âœ… ERROR tells us which DIRECTION to move")
    print("âœ… INPUT ensures we only blame what was ACTIVE")
    print("âœ… LEARNING_RATE controls how FAST we change")
    print("âœ… It's not magic - it's logical reasoning made mathematical!")
    print()
    print("ðŸš€ Now you understand the heart of machine learning!")
    print("Ready to see why this breaks down for XOR?")
